<!DOCTYPE html>
<html lang="en">
<head>
  <title>Build a Data Product with Databricks</title>
  <meta charset="utf-8">
  <meta name="description" content="Build a Data Product with Databricks"/>
  <meta name="keywords"
        content="data mesh, data mesh architecture, domain-driven data analytics, data analytics, domain-driven design, domain ownership, data as a product, data product, federated governance, self-serve data platform, data platform">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:site" content="@innoq"/>
  <meta name="twitter:title" content="Build a Data Product with Databricks"/>
  <meta name="twitter:description" content="In this article, we will use Data Contracts and the new Databricks Asset Bundles that are a great fit to implement data products."/>
  <meta name="twitter:image"
        content="https://www.datamesh-architecture.com/images/databricks-dataproduct-code.png"/>
  <meta property="og:url" content="https://datamesh-architecture.com"/>
  <meta property="og:title" content="Build a Data Product with Databricks"/>
  <meta property="og:description" content="In this article, we will use Data Contracts and the new Databricks Asset Bundles that are a great fit to implement data products."/>
  <meta property="og:image"
        content="https://www.datamesh-architecture.com/images/databricks-dataproduct-code.png"/>

  <link rel="preload" as="font" type="font/woff2"
        href="https://www.innoq.com/assets/MarkPro-Book.woff2?cachebuster=2" crossorigin="">
  <link rel="preload" as="font" type="font/woff2"
        href="https://www.innoq.com/assets/MarkPro-Bold.woff2?cachebuster=2" crossorigin="">
  <link rel="preload" as="font" type="font/woff2"
        href="https://www.innoq.com/assets/MarkPro-Heavy.woff2?cachebuster=2" crossorigin="">
  <link rel="stylesheet" href="../css/style.css"/>
  <link rel="stylesheet" href="../css/0.9.3_css_bulma.css"/>
  <link rel="stylesheet" href="../css/font-awesome_6.0.0_css_all.css"/>
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
  <link rel="stylesheet" href="../css/highlight_11.4.0.min.css">
</head>
<body>
<nav class="navbar is-dark" role="navigation" aria-label="dropdown navigation">
  <div class="container">
    <div class="navbar-brand">
                    <span class="navbar-burger" data-target="navbarMenuHeroA">
            <span></span>
            <span></span>
            <span></span>
          </span>
    </div>
    <div id="navbarMenuHeroA" class="navbar-menu">

      <div class="navbar-end">


        <a href="/#why" class="navbar-item">
          Why
        </a>
        <a href="/#what-is-data-mesh" class="navbar-item">
          What
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a href="/#how-to-design-a-data-mesh" class="navbar-link is-arrowless">
            How
          </a>
          <div class="navbar-dropdown" id="navbarMenuArchitectureDropdown">
            <a href="/#how-to-design-a-data-mesh" class="navbar-item">Data Mesh Architecture</a>
            <hr class="navbar-divider">
            <a href="/#data-product" class="navbar-item">Data Product</a>
            <a href="/#federated-governance" class="navbar-item">Federated Governance</a>
            <a href="/#analytical-data" class="navbar-item">Analytical Data</a>
            <a href="/#ingesting" class="navbar-item">Ingesting</a>
            <a href="/#clean-data" class="navbar-item">Clean Data</a>
            <a href="/#analytics" class="navbar-item">Analytics</a>
            <a href="/#data-platform" class="navbar-item">Data Platform</a>
            <a href="/#enabling-team" class="navbar-item">Enabling Team</a>
          </div>
        </div>
        <a href="/#mesh" class="navbar-item">
          Mesh
        </a>
        <div class="navbar-item has-dropdown is-hoverable">
          <div class="navbar-link is-arrowless">
            Specifications
          </div>
          <div class="navbar-dropdown" id="navbarMenuSpecificationsDropdown">
            <a href="https://www.dataproduct-specification.com" class="navbar-item">Data Product
              Specification</a>
            <a href="https://www.datacontract-specification.com" class="navbar-item">Data Contract
              Specification</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <div class="navbar-link is-arrowless">
            Open Source
          </div>
          <div class="navbar-dropdown" id="navbarMenuArchitectureToolsDropdown">
            <a href="/data-product-canvas.html" class="navbar-item">Data Product Canvas</a><a
            href="/data-mesh-canvas.html" class="navbar-item">Data Mesh Canvas</a>
            <a href="/fitness-test.html" class="navbar-item">Fitness Test</a><a
            href="https://cli.datacontract.com" class="navbar-item">Data Contract CLI</a>
            <a href="/open-source/aws.html" class="navbar-item">AWS Terraform Modules</a>
            <a href="/open-source/gcp.html" class="navbar-item">GCP Terraform Modules</a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a href="/#tech-stacks" class="navbar-link is-arrowless">
            Tech Stacks
          </a>
          <div class="navbar-dropdown" id="navbarMenuTechStackDropdown">
            <a href="/tech-stacks/google-cloud-bigquery.html" class="navbar-item">
              Google Cloud BigQuery
            </a>
            <a href="/tech-stacks/aws-s3-athena.html" class="navbar-item">
              AWS S3 and AWS Athena
            </a>
            <a href="/tech-stacks/azure-synapse-analytics.html" class="navbar-item">
              Azure Synapse Analytics
            </a>
            <a href="/tech-stacks/dbt-snowflake.html" class="navbar-item">
              dbt and Snowflake
            </a>
            <a href="/tech-stacks/databricks.html" class="navbar-item">
              Databricks
            </a>
            <a href="/tech-stacks/minio-trino.html" class="navbar-item">
              MinIO and Trino
            </a>
            <a href="/tech-stacks/sap.html" class="navbar-item">
              SAP
            </a>
            <a href="/tech-stacks/kafka-risingwave.html" class="navbar-item">
              Kafka and RisingWave
            </a>
          </div>
        </div>
        <div class="navbar-item has-dropdown is-hoverable">
          <a href="/#domain-teams-journey" class="navbar-link is-arrowless">
            Start the Journey
          </a>
          <div class="navbar-dropdown is-right" id="navbarMenuTransformationDropdown">
            <a href="/#domain-teams-journey" class="navbar-item">
              Domain Team’s Journey
            </a>
            <a href="/#data-teams-journey" class="navbar-item">
              Data Team’s Journey
            </a>
            <a href="/literature.html" class="navbar-item">
              Scientific Literature
            </a>
            <a href="/real-world-learnings.html" class="navbar-item">
              Real World Learnings
            </a>
            <a href="https://data-ai.innoq.com/services/data-mesh?ref=dma-nav" class="navbar-item">
              Get Help
            </a>
          </div>
        </div>
        <a href="https://www.datamesh-manager.com/?ref=dma-nav" class="navbar-item"
           style="color: #FF9B66">
          Data Mesh Manager
        </a>
      </div>
    </div>
  </div>
</nav>

<div class="container is-max-widescreen">

  <section class="section">

    <nav class="breadcrumb" aria-label="breadcrumbs">
      <ul>
        <li><a href="/">Data Mesh Architecture</a></li>
        <li><a href="/#tech-stacks">How To</a></li>
        <li class="is-active"><a href="#" aria-current="page">Build a Data Product with Databricks</a></li>
      </ul>
    </nav>

    <div class="content">


      <h1>How To Build a Data Product with Databricks</h1>

      <div class="media mb-4">
        <div class="">
          <figure class="image is-64x64">
            <img class="is-rounded" src="../images/jochen.webp" alt="Jochen Christ">
          </figure>
        </div>
        <div class="media-content">
          By <a href="https://www.innoq.com/en/staff/jochen-christ/">Jochen Christ</a><br>
          April 3, 2024
        </div>
      </div>

      <p>Today&#39;s data engineering shifted from building monolithic data pipeline structures to
        modular <em>data products</em>. </p>

      <figure>
        <img src="../images/dataproduct_components.png.webp" class="image" alt="Data Product Components">
        <figcaption>Data Product Components</figcaption>

      </figure>

      <p>A <a href="https://www.datamesh-architecture.com/#data-product">data product</a> is the deliverable that contains everything around a business concept to
        fulfill a data consumer&#39;s need:</p>
      <ul>
        <li>tables to actually store data</li>
        <li>code that transform data</li>
        <li>tests to verify and monitor that data is correct</li>
        <li>output ports to make data accessable</li>
        <li>input ports to ingest data from source systems or access other data products</li>
        <li>data contracts to describe the API</li>
        <li>documentation</li>
        <li>meta information, such as ownership</li>
      </ul>

      <p>A data product is usually managed in one Git repository.</p>

      <p>
        Databricks is one of the most popular modern data platforms, now how can we engineer a
        professional data product with Databricks?
      </p>

      <figure>
        <img src="../images/databricks-dataproduct-workflows.png" alt="A workflow job deployed as a Databricks Asset Bundle">
        <figcaption>A workflow job deployed as a Databricks Asset Bundle</figcaption>
      </figure>

      <p>
        In this article, we will use Data Contracts and the new Databricks Asset Bundles that are a
        great fit to implement data products. All source code of this example project is available
        on <a href="https://github.com/datamesh-architecture/databricks-dataproduct-example">GitHub</a>.
      </p>


      <h2 id="define-the-data-contract">Define the Data Contract</h2>

      <p>
        Before we start implementing, let's discuss and define the business requirements.
        What does our data consumer need from us, what is their use case, what do they expect as a
        data model.
        And we need to make sure, that we understand and share the same semantics, quality
        expectations, and expected service levels.
      </p>

      <blockquote>
        We call this approach <em>contract-first</em>.
        We start designing the interface of the provided data model and its metadata as a data
        contract.
        We use the data contract to drive the implementation.
      </blockquote>

      <p>
        In our example, the COO of an e-commerce company wants to know if there is an issue with
        articles that are not sold for a longer period, i.e., articles with no sale during the
        last three months, the so-called <em>shelf warmers</em>.
      </p>

      <p>
        In collaboration with the data consumer, we define a data contract as YAML, using the <a
        href="https://datacontract.com">Data Contract Specification</a>:
      </p>
      <pre><code class="language-yaml">dataContractSpecification: 0.9.3
id: urn:datacontract:fulfillment:stock-last-sales
info:
  title: Last Sales
  version: 1.0.0
  description: |
    The data model contains all articles that are in stock.
    For every article the last sale timestamp is defined.
  owner: Fulfillment
  contact:
    name: John Doe (Data Product Owner)
    url: https://teams.microsoft.com/l/channel/19%3Ad7X0bzOrUrZ-QAGu0nTxlWACe5HOQ-8Joql71A_00000%40thread.tacv2/General?groupId=4d213734-d5a1-4130-8024-00000000&tenantId=b000e7de-5c4d-41a2-9e67-00000000
servers:
  development:
    type: databricks
    host: dbc-abcdefgh-1234.cloud.databricks.com
    catalog: acme
    schema: stock-last-sales
terms:
  usage: >
    Data can be used for reports, analytics and machine learning use cases.
    Order may be linked and joined by other tables
  limitations: >
    Not suitable for real-time use cases.
  billing: free
  noticePeriod: P3M
models:
  articles:
    description: One record per article that is currently in stock
    type: table
    fields:
      sku:
        description: The article number (stock keeping unit)
        type: string
        primary: true
        pattern: ^[A-Za-z0-9]{8,14}$
        minLength: 8
        maxLength: 14
        example: "96385074"
      quantity:
        description: The total amount of articles that are currently in stock in all warehouses.
        type: long
        minimum: 1
        required: true
      last_sale_timestamp:
        description: The business timestamp in UTC when there was the last sale for this article. Null means that the article was never sold.
        type: timestamp
      processing_timestamp:
        description: The technical timestamp in UTC when this row was updated
        type: timestamp
        required: true
servicelevels:
  availability:
    percentage: 99.9%
  retention:
    period: 1 year
  freshness:
    threshold: 25 hours
    timestampField: articles.processing_timestamp
  frequency:
    description: Data is updated once a day
    type: batch
    cron: 0 0 * * *
examples:
  - type: csv
    data: |
      sku,quantity,last_sale_timestamp,processing_timestamp
      1234567890123,5,2024-02-25T16:16:30.171798,2024-03-25T16:16:30.171807
      2345678901234,10,,2024-03-25T15:16:30.171811
      3456789012345,15,2024-03-02T12:16:30.171814,2024-03-25T14:16:30.171816
      4567890123456,20,,2024-03-25T13:16:30.171817
      5678901234567,25,2024-03-08T08:16:30.171819,2024-03-25T12:16:30.171821
      6789012345678,30,,2024-03-25T11:16:30.171823
      7890123456789,35,2024-03-14T04:16:30.171824,2024-03-25T10:16:30.171826
      8901234567890,40,,2024-03-25T09:16:30.171830
      9012345678901,45,2024-03-20T00:16:30.171833,2024-03-25T08:16:30.171835
      0123456789012,50,,2024-03-25T07:16:30.171837
quality:
  type: SodaCL
  specification:
    checks for articles:
      - row_count > 1000</code></pre>

      <div class="has-text-centered mb-4 is-italic">datacontract.yaml</div>

      <p>
        The dataset will contain all articles that currently are in stock and it includes the <code>last_sale_timestamp</code>, the attribute that is most relevant for the COO.
        The COO can easily filter in their BI tool (such
        as PowerBI, redash, ...) for articles with last_sale_timestamp older than three months.
        Terms and service Level attributes make it clear that the dataset is update daily at midnight.
      </p>


      <h2 id="create-the-databricks-asset-bundle">Create the Databricks Asset Bundle</h2>

      <p>
        Now it is time to develop a data product that <em>implements</em> this data contract.
        Databricks recently added the concept of <a
          href="https://docs.databricks.com/en/dev-tools/bundles/index.html"><em>Databricks Asset Bundles</em></a>
        that are a great fit to structure and develop data products. As time of writing in March
        2024, they are in Public Preview, meaning ready for production-use.
      </p>

      <p>Databricks Asset Bundles include all the infrastructure and code files to actually
        deploy data transformations to Databricks:</p>
      <ul>
        <li>Infrastructure resources</li>
        <li>Workspace configuration</li>
        <li>Source files, such as notebooks and Python scripts</li>
        <li>Unit tests</li>
      </ul>

      <p>
        The Databricks CLI bundles these assets and deploys them to Databricks Platform, internally
        it uses Terraform. Asset Bundles are well-integrated into the Databricks Platform, e.g.,
        it is not possible to edit code or jobs directly in Databricks, which enables a strict
        version control of all code and pipeline configuration.
      </p>

      <p>
        Bundles are extremely useful, when you have multiple environments, such as dev, staging, and production.
        You can deploy the same bundle to multiple targets with different configurations.
      </p>

      <p>To create a bundle, let&#39;s init in a new bundle:</p>

      <pre><code>databricks bundle init</code></pre>

      We use this configuration:

      <ul>
        <li>Template to use: <strong>default-python</strong></li>
        <li>Unique name for this project: <strong>stock_last_sales</strong></li>
        <li>Include a stub (sample) notebook in 'stock_last_sales/src': <strong>yes</strong></li>
        <li>Include a stub (sample) Delta Live Tables pipeline in 'stock_last_sales/src':  <strong>no</strong></li>
        <li>Include a stub (sample) Python package in 'stock_last_sales/src': <strong>yes</strong></li>
      </ul>

      <p>When we look into the bundle structure, let&#39;s have a quick look at the most relevant
        files:</p>
      <ul>
        <li><strong>databricks.yml</strong> The bundle configuration and deployment targets</li>
        <li><strong>src/</strong> The folder for the transformation code</li>
        <li><strong>tests/</strong> The folder to place unit tests</li>
        <li><strong>resources/</strong> The job definition for the workflow definition</li>
      </ul>

      <blockquote>Note: We recommend to maintain an internal bundle as template that
        incorporates the company&#39;s naming conventions, global policies, best practices, and
        integrations.
      </blockquote>

      <p>With asset bundles, we can write our code locally in our preferred IDE, such as VS Code
        (using the Databricks extension for Visual Studio Code), PyCharm, or IntelliJ IDEA (using
        Databricks Connect).</p>

      <figure>
        <img src="../images/databricks-dataproduct-ide.png" alt="Databricks Asset Bundle in IntelliJ IDEA">
        <figcaption>Databricks Asset Bundle in IntelliJ IDEA</figcaption>
      </figure>


      <p>To set up a local Python environment, we can use venv and install the development
        dependencies:</p>

      <pre><code>python3.10 -m venv .venv
source .venv/bin/activate
pip install -r requirements-dev.txt</code></pre>


      <h2 id="generate-unity-catalog-table">Generate Unity Catalog Table</h2>

      <p>
        How do we organize the data for our data product?
        In this example, we use Unity Catalog to manage storage as managed tables.
        On an isolation level, we decide that one data product should represent one <em>schema</em> in Unity Catalog.
      </p>

      <p>
        We can leverage the data contract YAML to generate infrastructure code:
      </p>

      <p>
        The <em>model</em> defines the table structure of the target data model.
        With the <a href="https://cli.datacontract.com">Data Contract CLI</a> tool, we can generate
        the SQL DDL code for the CREATE TABLE statement.
      </p>

      <pre><code>datacontract export --format sql datacontract.yaml

-- Data Contract: urn:datacontract:fulfillment:stock-last-sales
-- SQL Dialect: databricks
CREATE OR REPLACE TABLE acme.stock_last_sales.articles (
  sku STRING primary key,
  quantity BIGINT not null,
  last_sale_timestamp TIMESTAMP,
  processing_timestamp TIMESTAMP not null
);</code></pre>

      <p>
        The <a href="https://cli.datacontract.com">Data Contract CLI</a> tool is also available as a Python Library
        <code>datacontract-cli</code>. So let&#39;s add it to the requirements-dev.txt and use it in
        directly in a Databricks notebook to actually create the table in Unity Catalog:
      </p>

      <figure>
        <img src="../images/databricks-dataproduct-create-table.png" alt="Use Data Contract CLI to create the Unity Catalog Table">
        <figcaption>Use Data Contract CLI to create the Unity Catalog Table</figcaption>
      </figure>

      The Unity Catalog tables is a managed tables that internally uses the Delta format for efficient storage.

      <figure>
        <img src="../images/databricks-dataproduct-unity-catalog.png" alt="The created table in Unity Catalog">
        <figcaption>The created table in Unity Catalog</figcaption>
      </figure>

      <h2 id="develop-transformation-code">Develop Transformation Code</h2>

      <p>
        Now, let's write the core transformation logic.
        With Python-based Databricks Asset Bundles, we can develop our data pipelines as:
      </p>

      <ul>
        <li>Databricks Notebooks,</li>
        <li>Delta Live Tables, or</li>
        <li>Python files</li>
      </ul>

      <p>
        In this data product, we'll write plain Python files for our core transformation logic that will be
        deployed as Wheel packages.
      </p>

      <p>
        Our transformation takes all available stocks that we get from an input port, such as the
        operational system that manages the current stock data, and left-joins the dataframe with
        the latest sales timestamp for every sku. The sales information are also an input port,
        e.g., another upstream data product provided by the checkout team.
        We store the resulting dataframe in the previously generated table structure.
      </p>

      <figure>
        <img src="../images/databricks-dataproduct-code.png" alt="Transformation code">
        <figcaption>Transformation code as plain Python code</figcaption>
      </figure>

      With that option, the code remains reusable, easy to test with unit tests, and we can run it
      on our local machines. As professional data engineers, we make sure that the
      <code>calculate_last_sales()</code> function works as expected by writing good unit tests.

      <figure>
        <img src="../images/databricks-dataproduct-unittest.png" alt="Unit Test">
        <figcaption>Unit Tests</figcaption>
      </figure>

      <p>
        We update the job configuration to run the Python code as a python_wheel_task and configure
        the scheduler and the appropriate compute cluster.
      </p>

      <pre><code># The main job for stock_last_sales.
resources:
  jobs:
    stock_last_sales_job:
      name: stock_last_sales_job

      schedule:
        # Run every day at midnight
        quartz_cron_expression: '0 0 0 * * ?'
        timezone_id: Europe/Amsterdam

      tasks:
        - task_key: create_unity_catalog_table
          job_cluster_key: job_cluster
          notebook_task:
            notebook_path: ../src/create_unity_catalog_table.ipynb
          libraries:
            - pypi:
                package: datacontract-cli

        - task_key: main_task
          depends_on:
            - task_key: create_unity_catalog_table
          job_cluster_key: job_cluster
          python_wheel_task:
            package_name: stock_last_sales
            entry_point: main
          libraries:
            - whl: ../dist/*.whl

      job_clusters:
        - job_cluster_key: job_cluster
          new_cluster:
            spark_version: 13.3.x-scala2.12
            node_type_id: i3.xlarge
            autoscale:
                min_workers: 1
                max_workers: 4
</code></pre>
      <div class="has-text-centered mb-4 is-italic">resources/stock_last_sales_job.yml</div>

      <p>When we are confident, we can deploy the bundle to our Databricks dev instances (manually for now):</p>

      <pre><code>databricks bundle deploy</code></pre>

      <p>And let's trigger a manual run of our workflow:</p>

      <pre><code>databricks bundle run stock_last_sales_job</code></pre>

      <p>
        In Databricks, we can see that the workflow run was successful:
      </p>

      <figure>
        <img src="../images/databricks-dataproduct-jobrun.png" alt="Job Run in Databricks">
        <figcaption>A job in Databricks</figcaption>
      </figure>

      <p>
        And we have data in our table that we created earlier.
      </p>


      <figure>
        <img src="../images/databricks-dataproduct-data.png" alt="The result of our data pipeline">
        <figcaption>The result of our data pipeline</figcaption>
      </figure>



      <h2 id="test-the-data-product">Test the Data Contract</h2>

      <p>
        We are not quite finished with our task. How do we know, that the data is correct?
        While we have unit tests that give us confidence on the transformation code, we also need an
        acceptance test to verify, that we implemented the agreed data contract correctly.
      </p>

      <p>
        For that, we can use the <a href="https://cli.datacontract.com">Data Contract CLI</a> tool to make this check:
      </p>

      <pre><code>export DATACONTRACT_DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/b053xxxxxxx
export DATACONTRACT_DATABRICKS_TOKEN=dapia1926f7c64b7595880909xxxxxxxxxx
datacontract test datacontract.yaml</code></pre>

      <p>
        The <code>datacontract</code> tool takes all the schema and format information from the <em>model</em>,
        the quality attributes, and the metadata, and compares them with the actual dataset. It
        reads the connection details from the <em>servers</em> section and connects to Databricks
        executes all the checks and gives a comprehensive overview.
      </p>

      <figure>
        <img src="../images/databricks-dataproduct-clitests.png" alt="Test Results" class="ml-1 mt-1 mb-1" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);">
        <figcaption>Data Contract Test Results</figcaption>
      </figure>

      <p>We want to execute this test with every pipeline run, so once again, let's make a Notebook task for the test:</p>

      <figure>
        <img src="../images/databricks-dataproduct-notebooktest.png" alt="Data Contract Test as Notebook" class="ml-1 mt-1 mb-1" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);">
        <figcaption>Data Contract Test as Notebook</figcaption>
      </figure>


      <h2 id="deploy-with-cicd">Deploy with CI/CD</h2>

      <p>
        To automatically test, deploy the Asset Bundle to Databricks, and finally run the job once,
        we set up a CI/CD pipeline in GitHub, using a <a href="https://docs.databricks.com/en/dev-tools/bundles/ci-cd.html">GitHub Action</a>.
      </p>

      <pre><code>name: "Deploy Databricks Assets Bundle"
on:
  workflow_dispatch:
  push:
    branches: [ "main" ]

jobs:
  test:
    name: "Run unit tests"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{matrix.python-version}}
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-dev.txt
      - name: Test with pytest
        run: pytest
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }}
          DATABRICKS_CLUSTER_ID: ${{ secrets.DATABRICKS_CLUSTER_ID }}

  deploy:
    name: "Deploy bundle to DEV"
    runs-on: ubuntu-latest

    needs:
      - test

    steps:
      - uses: actions/checkout@v4
      - uses: databricks/setup-cli@main
      - run: databricks bundle deploy
        working-directory: .
        env:
          DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }}
          DATABRICKS_BUNDLE_ENV: dev
  run_pipieline:
    name: "Run pipeline"
    runs-on: ubuntu-latest

    needs:
      - deploy

    steps:
      - uses: actions/checkout@v4
      - uses: databricks/setup-cli@main
      - run: databricks bundle run stock_last_sales_job --refresh-all
        working-directory: .
        env:
          DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }}
          DATABRICKS_BUNDLE_ENV: dev
      </code></pre>

      <div class="has-text-centered mb-4 is-italic">.github/workflows/deploy.yml</div>

      Now, every time we update the code, the Asset Bundle is automatically deployed to Databricks.

      <figure>
        <img src="../images/databricks-dataproduct-githubaction.png" alt="CI/CD workflow in GitHub" class="ml-1 mt-1 mb-1" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);">
        <figcaption>CI/CD workflow in GitHub</figcaption>
      </figure>

      <h2 id="publish-metadata">Publish Metadata</h2>

      <p>
        For others to find, understand, and trust data products, we want to register them in a data product registry.
      </p>

      <p>
        In this example, we use <a href="https://www.datamesh-manager.com/?ref=dma-databricks-dataproduct">Data Mesh Manager</a>,
        a platform to register, manage, and discover data products, data contracts, and global policies.
      </p>

      <figure>
        <img src="../images/databricks-dataproduct-datameshmanager.png" alt="Metadata in Data Mesh Manager" class="ml-1 mt-1 mb-1" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);">
        <figcaption>Metadata in Data Mesh Manager</figcaption>
      </figure>

      <p>
        Again, let's create a notebook task (or Python code task) to publish the metadata to Data Mesh Manager and add the task to our workflow.
        We can use <a href="https://docs.databricks.com/en/security/secrets/index.html">Databricks Secrets</a> to make the API Key available in Databricks.
      </p>

      <pre><code>databricks secrets create-scope datamesh_manager
databricks secrets put-secret datamesh_manager api_key</code></pre>

      <figure>
        <img src="../images/databricks-dataproduct-notebookpublish.png" alt="Publish Metadata" class="ml-1 mt-1 mb-1" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);">
        <figcaption>Publish Metadata</figcaption>
      </figure>


      <h2 id="summary">Summary</h2>

      <p>
        Now, the COO can connect to this table with a BI tool (such as PowerBI, Tableau, Redash, or withing Databricks) to answer their business question.
      </p>

      <figure>
        <img src="../images/databricks-dataproduct-analytics.png" alt="Analytics within Databricks" style="box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);">
        <figcaption>Analytics within Databricks</figcaption>
      </figure>

      <p>
        Databricks Asset Bundles are a great fit to develop professional data products on Databricks,
        as it bundles all the resources and configurations (code, tests, storage, compute, scheduler, metadata, ...)
        that are needed to provide high-quality datasets to data consumers.
      </p>
      <p>
        It is easy to integration Data Contracts for defining the requirements and the <a href="https://cli.datacontract.com">Data Contract CLI</a> to automate acceptance tests.
      </p>

      <p>
        Find the source code for the example project on <a href="https://github.com/datamesh-architecture/databricks-dataproduct-example">GitHub</a>.
      </p>


      <h2 id="learn-more">Learn More</h2>

      <ul>
        <li><a href="https://github.com/datamesh-architecture/databricks-dataproduct-example">Source Code for the examples project</a></li>
        <li><a href="https://docs.databricks.com/en/dev-tools/bundles/index.html/">Official databricks documentation on Databricks Asset Bundles</a></li>
        <li><a href="https://www.datamesh-architecture.com/tech-stacks/databricks">Databricks Tech Stack</a></li>
        <li><a href="https://datacontract.com">Data Contract Specification</a></li>
        <li><a href="https://cli.datacontract.com">Data Contract CLI</a></li>
        <li><a href="https://www.datamesh-manager.com.com">Data Mesh Manager</a></li>
      </ul>

      <h2 id="need-help">Need Help?</h2>
        INNOQ offers <a href="https://data-ai.innoq.com/en/services/data-mesh?ref=dma-databricks-dataproduct">Data Product Engineering Consulting</a> (Ad).


    </div>


  </section>

</div>

<footer class="footer">
  <div class="content has-text-centered">
    <p>
      <a href="https://www.innoq.ai">
        <img src="/images/supported-by-innoq--petrol-apricot.svg" alt="Supported by INNOQ"
             class="footer-logo" width="180"/>
      </a>
    </p>
    <p>
      <a href="https://www.innoq.com/en/topics/data-mesh-workshop?ref=dma-footer">Workshop</a>&nbsp
      <a href="https://www.socreatory.com/de/trainings/datamesh?ref=dma-footer">Training</a>&nbsp
      <a href="https://www.innoq.com/en/impressum/">Legal Notice</a>&nbsp
      <a href="https://www.innoq.com/en/datenschutz/">Privacy</a>
    </p>
  </div>
</footer>


<script src="/js/navigation.js"></script>

<script async defer src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
<noscript><img src="https://queue.simpleanalyticscdn.com/noscript.gif" alt=""
               referrerpolicy="no-referrer-when-downgrade"/></noscript>


<script src="../js/highlight.min.js"></script>
<script>hljs.highlightAll();</script>

<link rel="stylesheet" href="/css/glightbox.css"/>
<script src="/js/glightbox.js"></script>
<script type="text/javascript">
  const lightbox = GLightbox({});
</script>


<script src="/js/anchor.min.js"></script>
<script>anchors.add();</script>
</body>

</html>
